services:
  frontend:
    build:
      context: ./frontend
      args:
        VERSION: ${FE_VERSION}
    image: frontend:${FE_VERSION}
    ports:
      - "${FRONTEND_PORT}:${FRONTEND_PORT}"
    networks:
      - app_network
    environment:
      VERSION: ${FE_VERSION}
      FRONTEND_PORT: ${FRONTEND_PORT}

  backend:
    build: 
      context: ./backend
      args:
        VERSION: ${BE_VERSION}
    image: backend:${BE_VERSION}
    ports:
      - "${BACKEND_PORT}:${BACKEND_PORT}"
    networks:
      - app_network
    depends_on:
      - ollama
    environment:
      BACKEND_PORT: ${BACKEND_PORT}
      CORS_ORIGINS: '["http://localhost:${FRONTEND_PORT}"]'
      OLLAMA_HOST: "http://ollama:${OLLAMA_PORT}"
      OLLAMA_MODEL: ${OLLAMA_MODEL}
      CHUNK_SIZE: ${CHUNK_SIZE}
      TOKEN_ENCODING: ${TOKEN_ENCODING}
      PROMPT_FILE: ${PROMPT_FILE}
      HEALTH_CHECK_TIMEOUT: ${HEALTH_CHECK_TIMEOUT}
      OLLAMA_TIMEOUT: ${OLLAMA_TIMEOUT}
      DEFAULT_CHUNK_TIME: ${DEFAULT_CHUNK_TIME}
      PYTHONUNBUFFERED: 1

  ollama:
    build: ./ollama
    ports:
      - "${OLLAMA_PORT}:${OLLAMA_PORT}"
    networks:
      - app_network
    volumes:
      - ollama_data:/root/.ollama
    environment:
      OLLAMA_MODEL: ${OLLAMA_MODEL}
    deploy:
      resources:
        limits:
          memory: ${OLLAMA_MEMORY}

networks:
  app_network:
    driver: bridge

volumes:
  ollama_data:
    name: ollama_data
